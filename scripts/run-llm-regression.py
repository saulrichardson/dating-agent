#!/usr/bin/env python3
from __future__ import annotations

import argparse
import base64
import copy
import difflib
import hashlib
import json
import sys
from dataclasses import asdict
from datetime import datetime
from pathlib import Path
from typing import Any, Optional

REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from automation_service.mobile import live_hinge_agent as lha
from automation_service.mobile.env import ensure_dotenv_loaded
from automation_service.mobile.llm_judge import JudgeCache, judge_cache_key, judge_hinge_decision
from automation_service.mobile.llm_validation import validate_decision_output


def _parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(
        description=(
            "Run an offline LLM regression suite on a dataset of packets/screenshots.\n\n"
            "This script never touches Appium or a device. It only calls the LLM decision engine and validates outputs.\n"
            "It can optionally compare against a saved baseline and/or use an LLM-as-judge scorer for drift tolerance."
        )
    )
    p.add_argument("--dataset", required=True, help="Path to a hinge_llm_regression_case.v1 JSONL file")
    p.add_argument(
        "--profile-json",
        default="",
        help=(
            "Optional profile JSON path override (used if dataset lines do not include profile_ref). "
            "If omitted, each case must include profile_ref to a JSON file."
        ),
    )
    p.add_argument("--model", default="gpt-4.1-mini", help="Decision model (default gpt-4.1-mini)")
    p.add_argument("--temperature", type=float, default=0.1, help="Decision temperature (default 0.1)")
    p.add_argument("--timeout-s", type=float, default=30, help="Decision timeout seconds (default 30)")
    p.add_argument("--api-key-env", default="OPENAI_API_KEY", help="API key env var for decision calls")
    p.add_argument("--base-url", default="https://api.openai.com", help="Base URL for decision calls")
    p.add_argument("--include-screenshot", action="store_true", help="Include screenshots when present")
    p.add_argument(
        "--image-detail",
        default="low",
        choices=("low", "high"),
        help="Screenshot detail setting (default low)",
    )
    p.add_argument(
        "--max-observed-strings",
        type=int,
        default=120,
        help="Max observed_strings forwarded to LLM (default 120)",
    )
    p.add_argument("--max-cases", type=int, default=25, help="Max dataset cases to run (default 25)")
    p.add_argument("--repeat", type=int, default=1, help="Repeat decision N times per case (stability check)")
    p.add_argument(
        "--baseline",
        default="",
        help="Optional baseline JSONL path to compare against (generated by --write-baseline).",
    )
    p.add_argument(
        "--write-baseline",
        default="",
        help="Write a baseline JSONL to this path (overwrites). Useful for snapshot testing.",
    )
    p.add_argument(
        "--min-message-similarity",
        type=float,
        default=0.78,
        help="When baseline differs, require message similarity >= this ratio (default 0.78).",
    )
    p.add_argument("--judge", action="store_true", help="Enable LLM-as-judge scoring for tolerant drift checks.")
    p.add_argument("--judge-model", default="gpt-4.1-mini", help="Judge model (default gpt-4.1-mini)")
    p.add_argument("--judge-api-key-env", default="OPENAI_API_KEY", help="API key env var for judge calls")
    p.add_argument("--judge-base-url", default="https://api.openai.com", help="Base URL for judge calls")
    p.add_argument("--judge-timeout-s", type=float, default=30, help="Judge timeout seconds (default 30)")
    p.add_argument("--judge-max-calls", type=int, default=12, help="Max judge calls per run (default 12)")
    p.add_argument("--judge-min-score", type=int, default=70, help="Min judge overall score to treat drift as acceptable (default 70)")
    p.add_argument(
        "--judge-cache",
        default="artifacts/validation/judge_cache.jsonl",
        help="Path to judge JSONL cache (default artifacts/validation/judge_cache.jsonl)",
    )
    p.add_argument(
        "--report-path",
        default="",
        help="Optional report JSON path. Defaults to artifacts/validation/llm_regression_<ts>.json",
    )
    return p


def _now_tag() -> str:
    return datetime.now().strftime("%Y%m%d-%H%M%S")


def _read_jsonl(path: Path) -> list[dict[str, Any]]:
    out: list[dict[str, Any]] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line:
            continue
        row = json.loads(line)
        if not isinstance(row, dict):
            raise ValueError(f"Expected JSON object lines in {path}")
        out.append(row)
    return out


def _normalize_ws(text: Optional[str]) -> str:
    # Keep snapshots/diffs stable and ASCII-first (avoid curly quotes drift across models).
    t = " ".join((text or "").split()).strip()
    if not t:
        return ""
    t = (
        t.replace("’", "'")
        .replace("‘", "'")
        .replace("“", '"')
        .replace("”", '"')
        .replace("—", "-")
        .replace("–", "-")
        .replace("…", "...")
    )
    return t


def _message_similarity(a: Optional[str], b: Optional[str]) -> float:
    aa = _normalize_ws(a)
    bb = _normalize_ws(b)
    if not aa and not bb:
        return 1.0
    if not aa or not bb:
        return 0.0
    return difflib.SequenceMatcher(a=aa, b=bb).ratio()


def _load_case_profile(*, case: dict[str, Any], dataset_dir: Path, override_profile_path: str) -> lha.HingeAgentProfile:
    if override_profile_path:
        return lha._load_profile(override_profile_path)
    ref = case.get("profile_ref")
    if not isinstance(ref, str) or not ref.strip():
        raise ValueError("Case missing profile_ref and no --profile-json override was provided")
    path = (dataset_dir / ref).resolve()
    if not path.exists():
        raise ValueError(f"Case profile_ref not found: {path}")
    return lha._load_profile(str(path))


def _load_case_screenshot_bytes(*, case: dict[str, Any], dataset_dir: Path) -> Optional[bytes]:
    screenshot = case.get("screenshot")
    if isinstance(screenshot, dict):
        kind = screenshot.get("type")
        if kind == "none":
            return None
        if kind == "base64":
            raw = screenshot.get("base64")
            if isinstance(raw, str) and raw.strip():
                try:
                    return base64.b64decode(raw)
                except Exception:
                    return None
            return None
        if kind == "path":
            p_raw = screenshot.get("path")
            if not isinstance(p_raw, str) or not p_raw.strip():
                return None
            p = Path(p_raw)
            if not p.is_absolute():
                p = (dataset_dir / p).resolve()
            if not p.exists() or p.is_dir():
                return None
            if p.suffix.lower() != ".png":
                return None
            return p.read_bytes()

    # Back-compat for datasets that only have packet.packet_screenshot_path.
    packet = case.get("packet") if isinstance(case.get("packet"), dict) else {}
    p2 = packet.get("packet_screenshot_path")
    if isinstance(p2, str) and p2.strip():
        p = Path(p2).expanduser()
        if p.exists() and p.is_file() and p.suffix.lower() == ".png":
            return p.read_bytes()
    return None


def _load_baseline_index(path: Path) -> dict[tuple[str, str], dict[str, Any]]:
    out: dict[tuple[str, str], dict[str, Any]] = {}
    for row in _read_jsonl(path):
        case_id = row.get("case_id")
        model = row.get("model")
        if isinstance(case_id, str) and isinstance(model, str):
            out[(case_id, model)] = row
    return out


def _expected_any(case: dict[str, Any]) -> set[str]:
    expected = case.get("expected")
    if isinstance(expected, dict):
        xs = expected.get("expect_action_any")
        if isinstance(xs, list):
            return {str(x) for x in xs if isinstance(x, str) and str(x).strip()}
    return set()


def _require_message(case: dict[str, Any]) -> bool:
    expected = case.get("expected")
    if isinstance(expected, dict):
        return bool(expected.get("require_message"))
    return False


def main() -> int:
    args = _parser().parse_args()
    ensure_dotenv_loaded()

    dataset_path = Path(args.dataset).expanduser().resolve()
    if not dataset_path.exists():
        raise SystemExit(f"dataset not found: {dataset_path}")
    dataset_dir = dataset_path.parent

    cases = _read_jsonl(dataset_path)
    cases = cases[: max(0, int(args.max_cases))]

    decision_engine_dict = {
        "type": "llm",
        "llm_failure_mode": "fail",
        "llm": {
            "model": str(args.model),
            "temperature": float(args.temperature),
            "timeout_s": float(args.timeout_s),
            "api_key_env": str(args.api_key_env),
            "base_url": str(args.base_url),
            "include_screenshot": bool(args.include_screenshot),
            "image_detail": str(args.image_detail),
            "max_observed_strings": int(args.max_observed_strings),
        },
    }
    decision_engine = lha._parse_decision_engine(decision_engine_dict, context="run-llm-regression: decision_engine")

    baseline_index: dict[tuple[str, str], dict[str, Any]] = {}
    if args.baseline:
        baseline_path = Path(args.baseline).expanduser().resolve()
        if not baseline_path.exists():
            raise SystemExit(f"baseline not found: {baseline_path}")
        baseline_index = _load_baseline_index(baseline_path)

    write_baseline_path = None
    baseline_out_lines: list[str] = []
    if args.write_baseline:
        write_baseline_path = Path(args.write_baseline).expanduser().resolve()

    judge_cache = JudgeCache(path=Path(args.judge_cache).expanduser().resolve())
    judge_calls = 0

    per_case: list[dict[str, Any]] = []
    failures: list[str] = []
    validator_issue_counts: dict[str, int] = {}

    for idx, case in enumerate(cases, 1):
        if case.get("contract_version") != "hinge_llm_regression_case.v1":
            failures.append(f"case[{idx}]: unexpected contract_version={case.get('contract_version')!r}")
            continue

        case_id = case.get("case_id")
        if not isinstance(case_id, str) or not case_id.strip():
            failures.append(f"case[{idx}]: missing case_id")
            continue

        packet = case.get("packet")
        if not isinstance(packet, dict):
            failures.append(f"{case_id}: packet missing or not object")
            continue

        nl_query = case.get("nl_query")
        if not isinstance(nl_query, str) or not nl_query.strip():
            nl_query = None

        profile = _load_case_profile(case=case, dataset_dir=dataset_dir, override_profile_path=str(args.profile_json).strip())
        screenshot_bytes = _load_case_screenshot_bytes(case=case, dataset_dir=dataset_dir) if args.include_screenshot else None

        # Stability checks: run N times.
        decisions: list[dict[str, Any]] = []
        for _ in range(max(1, int(args.repeat))):
            action, reason, message_text, target_id, llm_trace = lha._llm_decide_with_trace(
                packet=copy.deepcopy(packet),
                profile=profile,
                decision_engine=decision_engine,
                nl_query=nl_query,
                screenshot_png_bytes=screenshot_bytes,
            )
            validation = validate_decision_output(
                action=action,
                reason=reason,
                message_text=message_text,
                target_id=target_id,
                packet=packet,
                profile=profile,
            )
            for issue in validation.issues:
                validator_issue_counts[issue] = validator_issue_counts.get(issue, 0) + 1
            decisions.append(
                {
                    "action": action,
                    "reason": reason,
                    "message_text": message_text,
                    "target_id": target_id,
                    "llm_trace": llm_trace,
                    "validation": asdict(validation),
                }
            )

        stable_actions = sorted(set(d["action"] for d in decisions))
        stable = len(stable_actions) == 1

        selected = decisions[0]
        action = selected["action"]
        message_text = selected["message_text"]
        selected_validation = selected.get("validation") if isinstance(selected.get("validation"), dict) else {}

        expected_any = _expected_any(case)
        if expected_any and action not in expected_any:
            failures.append(f"{case_id}: unexpected_action={action!r} expected_any={sorted(expected_any)}")

        if _require_message(case) and not (isinstance(message_text, str) and message_text.strip()):
            failures.append(f"{case_id}: expected message_text but got empty")

        if selected_validation.get("ok") is not True:
            issues = selected_validation.get("issues")
            failures.append(f"{case_id}: validation_failed issues={issues}")

        baseline_row = baseline_index.get((case_id, str(args.model)))
        baseline_cmp = None
        judge = None

        if baseline_row is not None:
            baseline_action = baseline_row.get("action")
            baseline_message = baseline_row.get("message_text")
            action_match = (action == baseline_action)
            msg_sim = _message_similarity(message_text, baseline_message)
            baseline_cmp = {
                "baseline_action": baseline_action,
                "baseline_message_text": baseline_message,
                "action_match": action_match,
                "message_similarity": msg_sim,
                "message_exact_match": (_normalize_ws(message_text) == _normalize_ws(baseline_message)),
            }

            drift_ok = True
            if not action_match:
                drift_ok = False

            if action == "send_message":
                if msg_sim < float(args.min_message_similarity):
                    drift_ok = False

                if args.judge and judge_calls < int(args.judge_max_calls):
                    key = judge_cache_key(
                        rubric_version="hinge_judge.v1",
                        judge_model=str(args.judge_model),
                        packet=packet,
                        profile=profile,
                        nl_query=nl_query,
                        action=action,
                        reason=str(selected.get("reason") or ""),
                        message_text=message_text,
                    )
                    cached = judge_cache.get(key)
                    if cached is not None:
                        judge = cached
                    else:
                        result, trace = judge_hinge_decision(
                            packet=packet,
                            profile=profile,
                            nl_query=nl_query,
                            action=action,
                            reason=str(selected.get("reason") or ""),
                            message_text=message_text,
                            judge_model=str(args.judge_model),
                            api_key_env=str(args.judge_api_key_env),
                            base_url=str(args.judge_base_url),
                            timeout_s=float(args.judge_timeout_s),
                        )
                        judge_calls += 1
                        judge = {"result": asdict(result), "trace": trace}
                        judge_cache.put(key, judge)

                    judge_score = int(((judge or {}).get("result") or {}).get("overall_score") or 0)
                    if judge_score >= int(args.judge_min_score):
                        # Judge can override baseline similarity drift for messages.
                        drift_ok = True if action_match else drift_ok

            if not drift_ok:
                failures.append(f"{case_id}: baseline_drift action={action!r} baseline={baseline_action!r}")

        # Write baseline output if requested.
        if write_baseline_path is not None:
            message_norm = None if message_text is None else _normalize_ws(message_text)
            row = {
                "contract_version": "hinge_llm_regression_baseline.v1",
                "created_at": datetime.now().isoformat(),
                "case_id": case_id,
                "model": str(args.model),
                "action": action,
                "target_id": selected.get("target_id"),
                "reason": selected.get("reason"),
                "message_text": message_norm,
                "message_sha256": hashlib.sha256(_normalize_ws(message_norm).encode("utf-8")).hexdigest(),
            }
            baseline_out_lines.append(json.dumps(row, ensure_ascii=False))

        per_case.append(
            {
                "idx": idx,
                "case_id": case_id,
                "screen_type": packet.get("screen_type"),
                "quality_score_v1": packet.get("quality_score_v1"),
                "nl_query": nl_query,
                "has_screenshot": bool(screenshot_bytes is not None),
                "stable_actions": stable,
                "unique_actions": stable_actions,
                "decisions": decisions,
                "baseline_compare": baseline_cmp,
                "judge": judge,
            }
        )

    if write_baseline_path is not None:
        write_baseline_path.parent.mkdir(parents=True, exist_ok=True)
        write_baseline_path.write_text("\n".join(baseline_out_lines) + ("\n" if baseline_out_lines else ""), encoding="utf-8")

    overall_ok = (not failures) and all((d["validation"]["ok"] for c in per_case for d in c.get("decisions", []) if isinstance(d, dict)))
    ts = _now_tag()
    default_report = (REPO_ROOT / "artifacts" / "validation" / f"llm_regression_{ts}.json").resolve()
    report_path = Path(args.report_path).expanduser().resolve() if args.report_path else default_report
    report_path.parent.mkdir(parents=True, exist_ok=True)

    payload = {
        "timestamp": datetime.now().isoformat(),
        "ok": bool(overall_ok),
        "dataset": str(dataset_path),
        "args": {
            "model": str(args.model),
            "temperature": float(args.temperature),
            "include_screenshot": bool(args.include_screenshot),
            "image_detail": str(args.image_detail),
            "max_cases": int(args.max_cases),
            "repeat": int(args.repeat),
            "baseline": (str(Path(args.baseline).expanduser().resolve()) if args.baseline else None),
            "write_baseline": (str(write_baseline_path) if write_baseline_path is not None else None),
            "min_message_similarity": float(args.min_message_similarity),
            "judge": bool(args.judge),
            "judge_model": str(args.judge_model),
            "judge_max_calls": int(args.judge_max_calls),
            "judge_min_score": int(args.judge_min_score),
            "judge_cache": str(Path(args.judge_cache).expanduser().resolve()),
        },
        "judge_calls": int(judge_calls),
        "validator_issue_counts": validator_issue_counts,
        "failures": failures,
        "cases": per_case,
    }
    report_path.write_text(json.dumps(payload, indent=2, ensure_ascii=False), encoding="utf-8")
    print(f"report={report_path}")
    return 0 if overall_ok else 2


if __name__ == "__main__":
    raise SystemExit(main())
